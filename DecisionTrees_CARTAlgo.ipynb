{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM22u8TGTb7KdWxJB5eFzA/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita1200/Machine-Learning-Topics/blob/main/DecisionTrees_CARTAlgo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CART Algo**\n",
        "1. CART constructs a Binary Tree.\n",
        "2. The primary challenge in the implementation of Decision Tree classifier is to identify the attribute that we consider as root node at each level.- process is called attribute selection technique. 2 approaches\n",
        "- Information gain - process assumes the attributes are categorical\n",
        "- Gini Index - assumes the attributes are continuous\n"
      ],
      "metadata": {
        "id": "CU7hXmDoMwzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algo -\n",
        "1. Decision Tree Node - A decision Tree node has Feature_index, threshold on the feature the divides the samples into left_half and right_half, a pointer to left_half, a pointer to right_half. If it is a leaf node, it will have a class label value.\n",
        "\n",
        "2. CART algo uses Gini Impurity. To calculate Gini Index need an array of class labels.\n",
        "\n",
        "3. To identify the feature,threshold that best leads to information gain/ reduces impurity, the algo iterates through all possible feature,threshold pairs and returns the one for which the gini-impurity is minimum.\n",
        "gini-impurity = left_half_szie* left_half_gini_index + right_half_szie*right_half_gini_index\n",
        "\n",
        "4. Process 3 is repeated at each node, until a base condition is met, at which the class with maximum count is returned as value of leaf node.(base condition - either num_of_classes==1/ num_of_smaples < min_num_of_samples-required for split)\n",
        "The tree is build recursivesly."
      ],
      "metadata": {
        "id": "q_QKg5sxQggG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cA_svaJlo-ed"
      },
      "outputs": [],
      "source": [
        "#from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing a Decision Tree Classifier**"
      ],
      "metadata": {
        "id": "Z2vpLWaPimlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a decision tree classifier Node\n",
        "class DecisionTreeNode:\n",
        "  def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n",
        "    self.feature_index = feature_index\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "\n",
        "  def isLeafNode(self):\n",
        "    return self.value is not None"
      ],
      "metadata": {
        "id": "m1Qnl116EVOu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate Gini impurity\n",
        "def gini_impurity(y):\n",
        "  gini = 1\n",
        "  labels = np.unique(y)\n",
        "  for cls in labels:\n",
        "    gini -= (np.sum(y==cls)/len(y))**2\n",
        "  return gini"
      ],
      "metadata": {
        "id": "7Z-IaKFZjdfq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best split\n",
        "def best_split(X,y):\n",
        "  best_gini = 1\n",
        "  best_feature_index = None\n",
        "  best_threshold = None\n",
        "\n",
        "  for feature_index in range(X.shape[1]):\n",
        "    thresholds = np.unique(X[:, feature_index])\n",
        "    for threshold in thresholds:\n",
        "      left_indices = X[:,feature_index] <= threshold\n",
        "      right_indices = X[:, feature_index] > threshold\n",
        "      if len(y[left_indices])==0 or len(y[right_indices])==0:\n",
        "        continue\n",
        "      gini_left = gini_impurity(y[left_indices])\n",
        "      gini_right = gini_impurity(y[right_indices])\n",
        "      gini = (np.sum(left_indices)*gini_left + np.sum(right_indices)*gini_right)/len(y)\n",
        "      if gini < best_gini:\n",
        "        best_gini = gini\n",
        "        best_feature_index = feature_index\n",
        "        best_threshold = threshold\n",
        "\n",
        "  return best_feature_index, best_threshold"
      ],
      "metadata": {
        "id": "Y8HW9xpTjfLx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Tree Recursively\n",
        "class DecisionTreeClassifierCustom:\n",
        "  def __init__(self, max_depth=100, min_samples_split=2):\n",
        "    self.max_depth = max_depth\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.root = None\n",
        "    self._is_fitted = False\n",
        "\n",
        "  def fit(self,X,y):\n",
        "    self.root = self._build_tree(X,y)\n",
        "    self._is_fitted = True\n",
        "\n",
        "  def _build_tree(self,X,y, depth=0):\n",
        "    num_of_samples, num_of_features = X.shape\n",
        "    num_of_labels = len(np.unique(y))\n",
        "    if depth >= self.max_depth or num_of_labels == 1 or num_of_samples < self.min_samples_split:\n",
        "      leaf_value = self._most_common_label(y)\n",
        "      return DecisionTreeNode(value=leaf_value)\n",
        "    feature_index, threshold = best_split(X,y)\n",
        "    if feature_index is None:\n",
        "      leaf_value = self._most_common_label(y)\n",
        "      return DecisionTreeNode(value=leaf_value)\n",
        "\n",
        "    left_indices = X[:,feature_index] <= threshold\n",
        "    right_indices = X[:, feature_index] > threshold\n",
        "    left = self._build_tree(X[left_indices], y[left_indices], depth+1)\n",
        "    right = self._build_tree(X[right_indices], y[right_indices], depth+1)\n",
        "    print(\"feature_index: \",feature_index)\n",
        "    print(\"threshold: \",threshold)\n",
        "    return DecisionTreeNode(feature_index,threshold,left,right)\n",
        "\n",
        "\n",
        "  def _most_common_label(self,y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    return labels[np.argmax(counts)]\n",
        "\n",
        "  def predict(self,x):\n",
        "    return self._traverse_tree(x,self.root)\n",
        "\n",
        "  def _traverse_tree(self,x,node):\n",
        "    if node.isLeafNode():\n",
        "      return node.value\n",
        "    if x[node.feature_index] <= node.threshold:\n",
        "      return self._traverse_tree(x,node.left)\n",
        "    return self._traverse_tree(x,node.right)\n",
        "\n",
        "  def print_tree(self, node, feature_names, class_names, depth=0):\n",
        "    indent = \"  \" * depth\n",
        "    if node.left is None and node.right is None:\n",
        "        print(f\"{indent}Predict class {class_names[node.value]}\")\n",
        "    else:\n",
        "        print(f\"{indent}If {feature_names[node.feature_index]} < {node.threshold:.2f}:\")\n",
        "        self.print_tree(node.left, feature_names, class_names, depth + 1)\n",
        "        print(f\"{indent}Else:\")\n",
        "        self.print_tree(node.right, feature_names, class_names, depth + 1)\n",
        ""
      ],
      "metadata": {
        "id": "HCzRv_nWjfka"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q34S7cy5Kkf9",
        "outputId": "795a2f41-e099-4121-e4bf-c6e391b041ca"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifierCustom()\n",
        "clf.fit(X,y)\n",
        "clf.print_tree(clf.root, iris.feature_names, iris.target_names)"
      ],
      "metadata": {
        "id": "IHYW8T9CKGbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6043a5-c8c1-4de4-e7d7-a1eb526cab6a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_index:  3\n",
            "threshold:  1.6\n",
            "feature_index:  0\n",
            "threshold:  6.7\n",
            "feature_index:  3\n",
            "threshold:  1.5\n",
            "feature_index:  2\n",
            "threshold:  4.9\n",
            "feature_index:  0\n",
            "threshold:  5.9\n",
            "feature_index:  2\n",
            "threshold:  4.8\n",
            "feature_index:  3\n",
            "threshold:  1.7\n",
            "feature_index:  2\n",
            "threshold:  1.9\n",
            "If petal length (cm) < 1.90:\n",
            "  Predict class setosa\n",
            "Else:\n",
            "  If petal width (cm) < 1.70:\n",
            "    If petal length (cm) < 4.90:\n",
            "      If petal width (cm) < 1.60:\n",
            "        Predict class versicolor\n",
            "      Else:\n",
            "        Predict class virginica\n",
            "    Else:\n",
            "      If petal width (cm) < 1.50:\n",
            "        Predict class virginica\n",
            "      Else:\n",
            "        If sepal length (cm) < 6.70:\n",
            "          Predict class versicolor\n",
            "        Else:\n",
            "          Predict class virginica\n",
            "  Else:\n",
            "    If petal length (cm) < 4.80:\n",
            "      If sepal length (cm) < 5.90:\n",
            "        Predict class versicolor\n",
            "      Else:\n",
            "        Predict class virginica\n",
            "    Else:\n",
            "      Predict class virginica\n"
          ]
        }
      ]
    }
  ]
}