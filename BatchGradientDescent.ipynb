{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7gN7Se+LZB/WI8xbRvGN4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita1200/Machine-Learning-Topics/blob/main/BatchGradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbxr-036ZBjp"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression model\n",
        "# y = w*x + b where w,b are parameters\n",
        "\n",
        "# sample training data\n",
        "x = np.random.rand(10,1)\n",
        "y = 2*x + np.random.rand()\n",
        "\n",
        "# hyperparameters - learning rate/ no of iterations/epochs\n",
        "learning_rate = 0.01\n",
        "iter = 500\n",
        "\n",
        "# initial value of parameters\n",
        "w = 0\n",
        "b = 0\n",
        "\n",
        "# loss function MSE (1/n)*(y - y_hat)**2 = (y-(w*x+b))**2\n",
        "# dldw -> 2*(y-(w*x+b))*(-x)\n",
        "# dldb -> 2*(y-(w*x+b))*(-b)"
      ],
      "metadata": {
        "id": "xeLLc177ZLIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def descend(x,y,w,b,learning_rate):\n",
        "  dldw = 0\n",
        "  dldb = 0\n",
        "  n = x.shape[0]\n",
        "  for xi,yi in zip(x,y):\n",
        "    y_pred = w*xi + b\n",
        "    dldw += -2*(yi- (w*xi + b))*(xi)\n",
        "    dldb += -2*(yi - (w*xi+b))\n",
        "  # parameter update\n",
        "  w -= learning_rate* dldw * (1/n)\n",
        "  b -= learning_rate * dldb * (1/n)\n",
        "  return w,b\n"
      ],
      "metadata": {
        "id": "N-mjUcV2ZOqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(iter):\n",
        "  w,b = descend(x,y,w,b,learning_rate)\n",
        "  y_pred = w*x + b\n",
        "  loss = np.mean((y-y_pred)**2)\n",
        "  print(f\"Epoch {epoch}, w: {w}, b: {b}, loss: {loss}\")"
      ],
      "metadata": {
        "id": "LwM8J0EFnGEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USVqn4p5lDdL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}