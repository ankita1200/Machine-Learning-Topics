{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhUgmcP6hkJXD6Geu8X4tr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankita1200/Machine-Learning-Topics/blob/main/StochasticGradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent\n",
        "1. The SGD estimates the gradient based on a randomaly selected data point.\n",
        "2. SGD is useful for large datasets\n",
        "3. Stochastic means random\n",
        "4. Gradient Descent is an optimization algorithm\n",
        "5. Its stochasity (randomness) can lead to slightly noisier process for convergence, resulting in model not settling for absolute minimum."
      ],
      "metadata": {
        "id": "tSU7TfM_NJvX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DGF33pCgM1d3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1,2,3,4,5])\n",
        "y = np.array([7,12,17,22.5,26.1])"
      ],
      "metadata": {
        "id": "5loXYKR6Qan7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y = mx + b\n",
        "# cost function F = ((mx+b)-y)^2\n",
        "# dF/dm = 2((mx+b)-y)*x\n",
        "# dF/db = 2*((mx+b)-y)\n",
        "\n",
        "# randomly initialize m and b\n",
        "m = np.random.randn()\n",
        "b = np.random.randn()\n",
        "learning_rate = 0.01\n",
        "\n",
        "print(m)\n",
        "print(b)\n",
        "\n",
        "epochs = 10\n",
        "for it in range(epochs):\n",
        "  # randomly select a data point to calculate gradient\n",
        "  idx = np.random.randint(len(x))\n",
        "  sample_x = x[idx]\n",
        "  sample_y = y[idx]\n",
        "  pred = m*sample_x + b\n",
        "  grad_m = 2*(pred-sample_y)*sample_x\n",
        "  grad_b = 2*(pred-sample_y)\n",
        "  m = m - learning_rate*grad_m\n",
        "  b = b - learning_rate*grad_b\n",
        "print(m)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOOMEIK4Qqjq",
        "outputId": "e7d69359-b6f4-4def-b745-e340e04274e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.1337583157864508\n",
            "1.129921333826694\n",
            "4.775520037317617\n",
            "2.2246694873174113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwxH8s1xViBw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}